{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    HfArgumentParser,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version\n",
    "from transformers.utils.versions import require_version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_min_version(\"4.13.0.dev0\")\n",
    "require_version(\n",
    "    \"datasets>=1.8.0\",\n",
    "    \"To fix: pip install -r examples/pytorch/translation/requirements.txt\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\n",
    "            \"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"\n",
    "        }\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"Pretrained config name or path if not the same as model_name\"\n",
    "        },\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"Pretrained tokenizer name or path if not the same as model_name\"\n",
    "        },\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"Where to store the pretrained models downloaded from huggingface.co\"\n",
    "        },\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"\n",
    "        },\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\n",
    "            \"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"\n",
    "        },\n",
    "    )\n",
    "    use_auth_token: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n",
    "            \"with private models).\"\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    source_lang: str = field(\n",
    "        default=None, metadata={\"help\": \"Source language id for translation.\"}\n",
    "    )\n",
    "    target_lang: str = field(\n",
    "        default=None, metadata={\"help\": \"Target language id for translation.\"}\n",
    "    )\n",
    "\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"},\n",
    "    )\n",
    "    e2e: Optional[bool] = (\n",
    "        field(default=False, metadata={\"help\": \"Prepare data for e2e\"}),\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The configuration name of the dataset to use (via the datasets library).\"\n",
    "        },\n",
    "    )\n",
    "    dataset_field_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The field name, useful when the data is stored as json.\"},\n",
    "    )\n",
    "    train_file: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The input training data file (a jsonlines).\"}\n",
    "    )\n",
    "    validation_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"An optional input evaluation data file to evaluate the metrics (sacreblue) on \"\n",
    "            \"a jsonlines file.\"\n",
    "        },\n",
    "    )\n",
    "    test_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"An optional input test data file to evaluate the metrics (sacreblue) on \"\n",
    "            \"a jsonlines file.\"\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Overwrite the cached training and evaluation sets\"},\n",
    "    )\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "    max_source_length: Optional[int] = field(\n",
    "        default=1024,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated, sequences shorter will be padded.\"\n",
    "        },\n",
    "    )\n",
    "    max_target_length: Optional[int] = field(\n",
    "        default=128,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total sequence length for target text after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated, sequences shorter will be padded.\"\n",
    "        },\n",
    "    )\n",
    "    val_max_target_length: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total sequence length for validation target text after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated, sequences shorter will be padded. Will default to `max_target_length`.\"\n",
    "            \"This argument is also used to override the ``max_length`` param of ``model.generate``, which is used \"\n",
    "            \"during ``evaluate`` and ``predict``.\"\n",
    "        },\n",
    "    )\n",
    "    pad_to_max_length: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to pad all samples to model maximum sentence length. \"\n",
    "            \"If False, will pad the samples dynamically when batching to the maximum length in the batch. More \"\n",
    "            \"efficient on GPU but very bad for TPU.\"\n",
    "        },\n",
    "    )\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "            \"value if set.\"\n",
    "        },\n",
    "    )\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
    "            \"value if set.\"\n",
    "        },\n",
    "    )\n",
    "    max_predict_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n",
    "            \"value if set.\"\n",
    "        },\n",
    "    )\n",
    "    num_beams: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"Number of beams to use for evaluation. This argument will be passed to ``model.generate``, \"\n",
    "            \"which is used during ``evaluate`` and ``predict``.\"\n",
    "        },\n",
    "    )\n",
    "    ignore_pad_token_for_loss: bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to ignore the tokens corresponding to padded labels in the loss computation or not.\"\n",
    "        },\n",
    "    )\n",
    "    source_prefix: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"A prefix to add before every source text (useful for T5 models).\"\n",
    "        },\n",
    "    )\n",
    "    forced_bos_token: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The token to force as the first generated token after the :obj:`decoder_start_token_id`.\"\n",
    "            \"Useful for multilingual models like :doc:`mBART <../model_doc/mbart>` where the first generated token \"\n",
    "            \"needs to be the target language token.(Usually it is the target language token)\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "\n",
    "        if self.train_file is not None:\n",
    "            extension = self.train_file.split(\".\")[-1]\n",
    "            assert extension == \"json\", \"`train_file` should be a json file.\"\n",
    "        if self.validation_file is not None:\n",
    "            extension = self.validation_file.split(\".\")[-1]\n",
    "            assert extension == \"json\", \"`validation_file` should be a json file.\"\n",
    "        if self.val_max_target_length is None:\n",
    "            self.val_max_target_length = self.max_target_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = HfArgumentParser(\n",
    "        (ModelArguments, DataTrainingArguments, Seq2SeqTrainingArguments)\n",
    "    )\n",
    "model_args, data_args, training_args = parser.parse_json_file(\n",
    "    json_file='data/config/t5_eval.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32101, 768)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "        model_args.config_name\n",
    "        if model_args.config_name\n",
    "        else model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        revision=model_args.model_revision,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_args.tokenizer_name\n",
    "    if model_args.tokenizer_name\n",
    "    else model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    use_fast=model_args.use_fast_tokenizer,\n",
    "    revision=model_args.model_revision,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    ")\n",
    "tokenizer.add_tokens([\"<hl>\"])\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "    config=config,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    revision=model_args.model_revision,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    ")\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adam\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\datasets\\dataset_dict.py:1244: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['name', 'sourceName', 'sourceUrl', 'faqLabels', 'answer', 'question'],\n",
       "        num_rows: 6309\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['name', 'sourceName', 'sourceUrl', 'faqLabels', 'answer', 'question'],\n",
       "        num_rows: 702\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = datasets.load_from_disk(dataset_path='data/t5_bobby_dataset')\n",
    "column_names = test_dataset[\"test\"].column_names\n",
    "padding = \"max_length\" if data_args.pad_to_max_length else False\n",
    "prefix = data_args.source_prefix if data_args.source_prefix is not None else \"\"\n",
    "max_target_length = data_args.max_target_length\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(samples):\n",
    "    \"\"\"\n",
    "    def construct_input(samples, i):\n",
    "        answer, context = samples[\"answers\"][i], samples[\"context\"][i]\n",
    "        if isinstance(answer, list):\n",
    "            answer = answer[0]\n",
    "        if isinstance(answer[\"text\"], str):\n",
    "            answer_text = answer[\"text\"].strip()\n",
    "        else:\n",
    "            answer_text = answer[\"text\"][0].strip()\n",
    "\n",
    "        hl_answer = f\"<hl>{answer_text}<hl>\"\n",
    "        return context.replace(answer_text, hl_answer)\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    targets = []\n",
    "\n",
    "    #if not data_args.e2e:\n",
    "    for i in range(len(samples[\"answer\"])):\n",
    "        inputs.append(samples[\"answer\"][i])\n",
    "        targets.append(samples[\"question\"][i])\n",
    "    #else:\n",
    "    #    for i in range(len(samples[\"answer\"])):\n",
    "    #        inputs.append(samples[\"answer\"][i])\n",
    "    #        targets.append(\" <sep> \".join(samples[\"questions\"][i]))\n",
    "\n",
    "    inputs = [prefix + inp for inp in inputs]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=data_args.max_source_length,\n",
    "        padding=padding,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets, max_length=max_target_length, padding=padding, truncation=True\n",
    "        )\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\" and data_args.ignore_pad_token_for_loss:\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label]\n",
    "            for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at d:\\GitHub\\bachelor-thesis\\data\\t5_bobby_dataset\\test\\cache-5993eb26984c59f5.arrow\n"
     ]
    }
   ],
   "source": [
    "if \"test\" not in test_dataset:\n",
    "    raise ValueError(\"--do_predict requires a test dataset\")\n",
    "predict_dataset = test_dataset[\"test\"]\n",
    "if data_args.max_predict_samples is not None:\n",
    "    predict_dataset = predict_dataset.select(\n",
    "        range(data_args.max_predict_samples)\n",
    "    )\n",
    "with training_args.main_process_first(\n",
    "    desc=\"prediction dataset map pre-processing\"\n",
    "):\n",
    "    predict_dataset = predict_dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=data_args.preprocessing_num_workers,\n",
    "        remove_columns=column_names,\n",
    "        load_from_cache_file=not data_args.overwrite_cache,\n",
    "        desc=\"Running tokenizer on prediction dataset\",\n",
    "    )\n",
    "predict_dataset.set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 702\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wer sollte eine Gutschein-Nachnahme veranlassen, wenn ich mein Kind nicht in die Kita betrete?\tWas passiert, wenn mein Kind nicht in die Kita kommt?\tWas ist bei einer l√§ngerfristigen Nichtnutzung des Kitaplatzes zu tun?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "results = []\n",
    "input_ids = predict_dataset[0]['input_ids'].unsqueeze(0)\n",
    "attention_mask = predict_dataset[0]['attention_mask'].unsqueeze(0)\n",
    "q1, q2, q3 = model.generate(input_ids=input_ids,attention_mask=attention_mask,max_length=64,num_return_sequences=3, do_sample=True)\n",
    "predictions = [q1, q2, q3]\n",
    "predictions = [\n",
    "    tokenizer.decode(ids, skip_special_tokens=True)\n",
    "    for ids in predictions\n",
    "]\n",
    "print(\"\\t\".join(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for row in tqdm(predict_dataset):\n",
    "    input_ids = row['input_ids'].unsqueeze(0)\n",
    "    attention_mask = row['attention_mask'].unsqueeze(0)\n",
    "    q1, q2, q3 = model.generate(input_ids=input_ids,attention_mask=attention_mask,max_length=64,num_return_sequences=3, do_sample=True)\n",
    "    predictions = [q1, q2, q3]\n",
    "    predictions = [\n",
    "        tokenizer.decode(ids, skip_special_tokens=True)\n",
    "        for ids in predictions\n",
    "    ]\n",
    "    \n",
    "    results.append(\"\\t\".join(predictions))\n",
    "\n",
    "with open(f\"data/t5results/t5_predictions.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

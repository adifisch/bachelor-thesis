{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adam\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This Training script is based on the Training script of the huggingface user dehio.\n",
    "It is adjusted to fit the needs of this bachelors thesisÂ´.\n",
    "Please see the original training script here: https://github.com/d-e-h-i-o/german-qg/blob/main/run_qg.py\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    HfArgumentParser,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version\n",
    "from transformers.utils.versions import require_version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_min_version(\"4.13.0.dev0\")\n",
    "require_version(\n",
    "    \"datasets>=1.8.0\",\n",
    "    \"To fix: pip install -r examples/pytorch/translation/requirements.txt\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\n",
    "            \"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"\n",
    "        }\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"Pretrained config name or path if not the same as model_name\"\n",
    "        },\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"Pretrained tokenizer name or path if not the same as model_name\"\n",
    "        },\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"Where to store the pretrained models downloaded from huggingface.co\"\n",
    "        },\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"\n",
    "        },\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\n",
    "            \"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"\n",
    "        },\n",
    "    )\n",
    "    use_auth_token: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n",
    "            \"with private models).\"\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    source_lang: str = field(\n",
    "        default=None, metadata={\"help\": \"Source language id for translation.\"}\n",
    "    )\n",
    "    target_lang: str = field(\n",
    "        default=None, metadata={\"help\": \"Target language id for translation.\"}\n",
    "    )\n",
    "\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"},\n",
    "    )\n",
    "    e2e: Optional[bool] = (\n",
    "        field(default=False, metadata={\"help\": \"Prepare data for e2e\"}),\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The configuration name of the dataset to use (via the datasets library).\"\n",
    "        },\n",
    "    )\n",
    "    dataset_field_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The field name, useful when the data is stored as json.\"},\n",
    "    )\n",
    "    train_file: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The input training data file (a jsonlines).\"}\n",
    "    )\n",
    "    validation_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"An optional input evaluation data file to evaluate the metrics (sacreblue) on \"\n",
    "            \"a jsonlines file.\"\n",
    "        },\n",
    "    )\n",
    "    test_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"An optional input test data file to evaluate the metrics (sacreblue) on \"\n",
    "            \"a jsonlines file.\"\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Overwrite the cached training and evaluation sets\"},\n",
    "    )\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "    max_source_length: Optional[int] = field(\n",
    "        default=1024,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated, sequences shorter will be padded.\"\n",
    "        },\n",
    "    )\n",
    "    max_target_length: Optional[int] = field(\n",
    "        default=128,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total sequence length for target text after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated, sequences shorter will be padded.\"\n",
    "        },\n",
    "    )\n",
    "    val_max_target_length: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total sequence length for validation target text after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated, sequences shorter will be padded. Will default to `max_target_length`.\"\n",
    "            \"This argument is also used to override the ``max_length`` param of ``model.generate``, which is used \"\n",
    "            \"during ``evaluate`` and ``predict``.\"\n",
    "        },\n",
    "    )\n",
    "    pad_to_max_length: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to pad all samples to model maximum sentence length. \"\n",
    "            \"If False, will pad the samples dynamically when batching to the maximum length in the batch. More \"\n",
    "            \"efficient on GPU but very bad for TPU.\"\n",
    "        },\n",
    "    )\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "            \"value if set.\"\n",
    "        },\n",
    "    )\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
    "            \"value if set.\"\n",
    "        },\n",
    "    )\n",
    "    max_predict_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n",
    "            \"value if set.\"\n",
    "        },\n",
    "    )\n",
    "    num_beams: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"Number of beams to use for evaluation. This argument will be passed to ``model.generate``, \"\n",
    "            \"which is used during ``evaluate`` and ``predict``.\"\n",
    "        },\n",
    "    )\n",
    "    ignore_pad_token_for_loss: bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to ignore the tokens corresponding to padded labels in the loss computation or not.\"\n",
    "        },\n",
    "    )\n",
    "    source_prefix: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"A prefix to add before every source text (useful for T5 models).\"\n",
    "        },\n",
    "    )\n",
    "    forced_bos_token: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The token to force as the first generated token after the :obj:`decoder_start_token_id`.\"\n",
    "            \"Useful for multilingual models like :doc:`mBART <../model_doc/mbart>` where the first generated token \"\n",
    "            \"needs to be the target language token.(Usually it is the target language token)\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "\n",
    "        if self.train_file is not None:\n",
    "            extension = self.train_file.split(\".\")[-1]\n",
    "            assert extension == \"json\", \"`train_file` should be a json file.\"\n",
    "        if self.validation_file is not None:\n",
    "            extension = self.validation_file.split(\".\")[-1]\n",
    "            assert extension == \"json\", \"`validation_file` should be a json file.\"\n",
    "        if self.val_max_target_length is None:\n",
    "            self.val_max_target_length = self.max_target_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = HfArgumentParser(\n",
    "        (ModelArguments, DataTrainingArguments, Seq2SeqTrainingArguments)\n",
    "    )\n",
    "model_args, data_args, training_args = parser.parse_json_file(\n",
    "    json_file='data/config/t5_training.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/01/2023 22:17:21 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "10/01/2023 22:17:21 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=8,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=passive,\n",
      "log_on_each_node=True,\n",
      "logging_dir=models/t5-qg-bobby-2023-10-01\\runs\\Oct01_22-17-21_AdamTower,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=100,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=models/t5-qg-bobby-2023-10-01,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=2,\n",
      "per_device_train_batch_size=2,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=models/t5-qg-bobby-2023-10-01,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=no,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    )\n",
    "\n",
    "log_level = training_args.get_process_log_level()\n",
    "logger.setLevel(log_level)\n",
    "datasets.utils.logging.set_verbosity(log_level)\n",
    "transformers.logging.set_verbosity(log_level)\n",
    "transformers.logging.enable_default_handler()\n",
    "transformers.logging.enable_explicit_format()\n",
    "logger.warning(\n",
    "        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    "    )\n",
    "logger.info(f\"Training/evaluation parameters {training_args}\")\n",
    "set_seed(training_args.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/01/2023 22:17:22 - WARNING - datasets.builder - Using custom data configuration default-a69777fe21ba2ca4\n",
      "10/01/2023 22:17:22 - INFO - datasets.info - Loading Dataset Infos from C:\\Users\\Adam\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\datasets\\packaged_modules\\csv\n",
      "10/01/2023 22:17:22 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "10/01/2023 22:17:22 - INFO - datasets.info - Loading Dataset info from C:\\Users\\Adam\\.cache\\huggingface\\datasets\\csv\\default-a69777fe21ba2ca4\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\n",
      "10/01/2023 22:17:22 - WARNING - datasets.builder - Found cached dataset csv (C:/Users/Adam/.cache/huggingface/datasets/csv/default-a69777fe21ba2ca4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "10/01/2023 22:17:22 - INFO - datasets.info - Loading Dataset info from C:/Users/Adam/.cache/huggingface/datasets/csv/default-a69777fe21ba2ca4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\n",
      "10/01/2023 22:17:22 - WARNING - datasets.arrow_dataset - Loading cached split indices for dataset at C:\\Users\\Adam\\.cache\\huggingface\\datasets\\csv\\default-a69777fe21ba2ca4\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-2b043fa47ec843b6.arrow and C:\\Users\\Adam\\.cache\\huggingface\\datasets\\csv\\default-a69777fe21ba2ca4\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-b23063bd2f906a06.arrow\n",
      "10/01/2023 22:17:22 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at C:\\Users\\Adam\\.cache\\huggingface\\datasets\\csv\\default-a69777fe21ba2ca4\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-baa47233abc8a359.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/01/2023 22:17:22 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at C:\\Users\\Adam\\.cache\\huggingface\\datasets\\csv\\default-a69777fe21ba2ca4\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-983a6e14a85f328c.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"csv\", split='train', data_files=\"data/t5_dataset.csv\")\n",
    "raw_datasets = raw_datasets.train_test_split(test_size=0.1)\n",
    "raw_datasets.save_to_disk('data/t5_bobby_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adam\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\datasets\\dataset_dict.py:1244: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['name', 'sourceName', 'sourceUrl', 'faqLabels', 'answer', 'question'],\n",
       "        num_rows: 6309\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['name', 'sourceName', 'sourceUrl', 'faqLabels', 'answer', 'question'],\n",
       "        num_rows: 702\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = datasets.load_from_disk(dataset_path='data/t5_bobby_dataset')\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:654] 2023-10-01 22:17:23,620 >> loading configuration file config.json from cache at cache\\models--dehio--german-qg-t5-quad\\snapshots\\e5eeeeaef49576b5679469f2d186971e4f647ea7\\config.json\n",
      "[INFO|configuration_utils.py:706] 2023-10-01 22:17:23,632 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"dehio/german-qg-t5-quad\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 32,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"\"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 32,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"\"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 32,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"\"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 32,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"\"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32101\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1799] 2023-10-01 22:17:24,054 >> loading file spiece.model from cache at cache\\models--dehio--german-qg-t5-quad\\snapshots\\e5eeeeaef49576b5679469f2d186971e4f647ea7\\spiece.model\n",
      "[INFO|tokenization_utils_base.py:1799] 2023-10-01 22:17:24,056 >> loading file tokenizer.json from cache at cache\\models--dehio--german-qg-t5-quad\\snapshots\\e5eeeeaef49576b5679469f2d186971e4f647ea7\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1799] 2023-10-01 22:17:24,057 >> loading file added_tokens.json from cache at cache\\models--dehio--german-qg-t5-quad\\snapshots\\e5eeeeaef49576b5679469f2d186971e4f647ea7\\added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:1799] 2023-10-01 22:17:24,058 >> loading file special_tokens_map.json from cache at cache\\models--dehio--german-qg-t5-quad\\snapshots\\e5eeeeaef49576b5679469f2d186971e4f647ea7\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1799] 2023-10-01 22:17:24,059 >> loading file tokenizer_config.json from cache at cache\\models--dehio--german-qg-t5-quad\\snapshots\\e5eeeeaef49576b5679469f2d186971e4f647ea7\\tokenizer_config.json\n",
      "[INFO|modeling_utils.py:2204] 2023-10-01 22:17:24,164 >> loading weights file pytorch_model.bin from cache at cache\\models--dehio--german-qg-t5-quad\\snapshots\\e5eeeeaef49576b5679469f2d186971e4f647ea7\\pytorch_model.bin\n",
      "[INFO|modeling_utils.py:2708] 2023-10-01 22:17:26,677 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:2717] 2023-10-01 22:17:26,679 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at dehio/german-qg-t5-quad.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32101, 768)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "        model_args.config_name\n",
    "        if model_args.config_name\n",
    "        else model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        revision=model_args.model_revision,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_args.tokenizer_name\n",
    "    if model_args.tokenizer_name\n",
    "    else model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    use_fast=model_args.use_fast_tokenizer,\n",
    "    revision=model_args.model_revision,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    ")\n",
    "tokenizer.add_tokens([\"<hl>\"])\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "    config=config,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    revision=model_args.model_revision,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    ")\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "last_checkpoint = None\n",
    "if model.config.decoder_start_token_id is None:\n",
    "    raise ValueError(\n",
    "        \"Make sure that `config.decoder_start_token_id` is correctly defined\"\n",
    "    )\n",
    "\n",
    "prefix = data_args.source_prefix if data_args.source_prefix is not None else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_args.do_train:\n",
    "        column_names = raw_datasets[\"train\"].column_names\n",
    "elif training_args.do_eval:\n",
    "    column_names = raw_datasets[\"validation\"].column_names\n",
    "elif training_args.do_predict:\n",
    "    column_names = raw_datasets[\"test\"].column_names\n",
    "else:\n",
    "    logger.info(\n",
    "        \"There is nothing to do. Please pass `do_train`, `do_eval` and/or `do_predict`.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_target_length = data_args.max_target_length\n",
    "padding = \"max_length\" if data_args.pad_to_max_length else False\n",
    "if training_args.label_smoothing_factor > 0 and not hasattr(\n",
    "    model, \"prepare_decoder_input_ids_from_labels\"\n",
    "):\n",
    "    logger.warning(\n",
    "        \"label_smoothing is enabled but the `prepare_decoder_input_ids_from_labels` method is not defined for\"\n",
    "        f\"`{model.__class__.__name__}`. This will lead to loss being calculated twice and will take up more memory\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(samples):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "\n",
    "    for i in range(len(samples[\"answer\"])):\n",
    "        inputs.append(samples[\"answer\"][i])\n",
    "        targets.append(samples[\"question\"][i])\n",
    "\n",
    "    inputs = [prefix + inp for inp in inputs]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=data_args.max_source_length,\n",
    "        padding=padding,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets, max_length=max_target_length, padding=padding, truncation=True\n",
    "        )\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\" and data_args.ignore_pad_token_for_loss:\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label]\n",
    "            for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on train dataset:   0%|          | 0/7 [00:00<?, ?ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/01/2023 22:17:27 - INFO - datasets.arrow_dataset - Caching processed dataset at C:\\Users\\Adam\\.cache\\huggingface\\datasets\\csv\\default-a69777fe21ba2ca4\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-b5ff44a89fd06d54.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adam\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\transformers\\tokenization_utils_base.py:3579: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  \"`as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your \"\n",
      "Running tokenizer on train dataset: 100%|ââââââââââ| 7/7 [00:01<00:00,  3.64ba/s]\n"
     ]
    }
   ],
   "source": [
    "if training_args.do_train:\n",
    "    if \"train\" not in raw_datasets:\n",
    "        raise ValueError(\"--do_train requires a train dataset\")\n",
    "    train_dataset = raw_datasets[\"train\"]\n",
    "    if data_args.max_train_samples is not None:\n",
    "        train_dataset = train_dataset.select(range(data_args.max_train_samples))\n",
    "    with training_args.main_process_first(desc=\"train dataset map pre-processing\"):\n",
    "        train_dataset = train_dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            remove_columns=column_names,\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on train dataset\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_args.do_eval:\n",
    "    max_target_length = data_args.val_max_target_length\n",
    "    if \"validation\" not in raw_datasets:\n",
    "        raise ValueError(\"--do_eval requires a validation dataset\")\n",
    "    eval_dataset = raw_datasets[\"validation\"]\n",
    "    if data_args.max_eval_samples is not None:\n",
    "        eval_dataset = eval_dataset.select(range(data_args.max_eval_samples))\n",
    "    with training_args.main_process_first(\n",
    "        desc=\"validation dataset map pre-processing\"\n",
    "    ):\n",
    "        eval_dataset = eval_dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            remove_columns=column_names,\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on validation dataset\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on prediction dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/01/2023 22:17:29 - INFO - datasets.arrow_dataset - Caching processed dataset at C:\\Users\\Adam\\.cache\\huggingface\\datasets\\csv\\default-a69777fe21ba2ca4\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-34be345d700e6cdc.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on prediction dataset: 100%|ââââââââââ| 1/1 [00:00<00:00,  4.88ba/s]\n"
     ]
    }
   ],
   "source": [
    "if training_args.do_predict:\n",
    "    max_target_length = data_args.val_max_target_length\n",
    "    if \"test\" not in raw_datasets:\n",
    "        raise ValueError(\"--do_predict requires a test dataset\")\n",
    "    predict_dataset = raw_datasets[\"test\"]\n",
    "    if data_args.max_predict_samples is not None:\n",
    "        predict_dataset = predict_dataset.select(\n",
    "            range(data_args.max_predict_samples)\n",
    "        )\n",
    "    with training_args.main_process_first(\n",
    "        desc=\"prediction dataset map pre-processing\"\n",
    "    ):\n",
    "        predict_dataset = predict_dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            remove_columns=column_names,\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on prediction dataset\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adam\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\ipykernel_launcher.py:15: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ð¤ Evaluate: https://huggingface.co/docs/evaluate\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "label_pad_token_id = (\n",
    "        -100 if data_args.ignore_pad_token_for_loss else tokenizer.pad_token_id\n",
    "    )\n",
    "if data_args.pad_to_max_length:\n",
    "    data_collator = default_data_collator\n",
    "else:\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=label_pad_token_id,\n",
    "        pad_to_multiple_of=8 if training_args.fp16 else None,\n",
    "    )\n",
    "\n",
    "# Metric\n",
    "metric = load_metric(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    if data_args.ignore_pad_token_for_loss:\n",
    "        # Replace -100 in the labels as we can't decode them.\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n",
    "    ]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset if training_args.do_train else None,\n",
    "    eval_dataset=eval_dataset if training_args.do_eval else None,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    "    if training_args.predict_with_generate\n",
    "    else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adam\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\transformers\\optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "[INFO|trainer.py:1634] 2023-10-01 22:17:31,721 >> ***** Running training *****\n",
      "[INFO|trainer.py:1635] 2023-10-01 22:17:31,722 >>   Num examples = 6309\n",
      "[INFO|trainer.py:1636] 2023-10-01 22:17:31,723 >>   Num Epochs = 10\n",
      "[INFO|trainer.py:1637] 2023-10-01 22:17:31,724 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:1638] 2023-10-01 22:17:31,724 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer.py:1639] 2023-10-01 22:17:31,725 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:1640] 2023-10-01 22:17:31,725 >>   Total optimization steps = 3940\n",
      "[INFO|trainer.py:1642] 2023-10-01 22:17:31,728 >>   Number of trainable parameters = 222882816\n",
      "  0%|          | 0/3940 [00:00<?, ?it/s][WARNING|logging.py:281] 2023-10-01 22:17:31,881 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  3%|â         | 100/3940 [02:27<1:29:16,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4208, 'learning_rate': 9.746192893401017e-05, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|â         | 200/3940 [04:48<1:25:34,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1888, 'learning_rate': 9.49238578680203e-05, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â         | 300/3940 [07:14<1:32:58,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0801, 'learning_rate': 9.238578680203046e-05, 'epoch': 0.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â         | 400/3940 [09:39<1:22:52,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9914, 'learning_rate': 8.984771573604062e-05, 'epoch': 1.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|ââ        | 500/3940 [12:12<1:29:40,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7216, 'learning_rate': 8.730964467005075e-05, 'epoch': 1.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|ââ        | 600/3940 [14:45<1:27:07,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7431, 'learning_rate': 8.477157360406092e-05, 'epoch': 1.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|ââ        | 700/3940 [17:13<1:14:04,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6866, 'learning_rate': 8.223350253807108e-05, 'epoch': 1.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|ââ        | 800/3940 [19:42<1:24:08,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5969, 'learning_rate': 7.969543147208121e-05, 'epoch': 2.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|âââ       | 900/3940 [22:10<1:09:00,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4498, 'learning_rate': 7.715736040609137e-05, 'epoch': 2.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|âââ       | 1000/3940 [24:33<1:09:50,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4432, 'learning_rate': 7.461928934010153e-05, 'epoch': 2.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|âââ       | 1100/3940 [26:51<1:05:14,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4433, 'learning_rate': 7.208121827411168e-05, 'epoch': 2.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|âââ       | 1200/3940 [29:13<1:12:57,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3827, 'learning_rate': 6.954314720812183e-05, 'epoch': 3.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|ââââ      | 1300/3940 [31:44<1:16:12,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2969, 'learning_rate': 6.700507614213199e-05, 'epoch': 3.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|ââââ      | 1400/3940 [34:29<1:13:50,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2737, 'learning_rate': 6.446700507614213e-05, 'epoch': 3.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|ââââ      | 1500/3940 [37:13<1:01:44,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2449, 'learning_rate': 6.192893401015228e-05, 'epoch': 3.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|ââââ      | 1600/3940 [39:59<59:34,  1.53s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.223, 'learning_rate': 5.939086294416244e-05, 'epoch': 4.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|âââââ     | 1700/3940 [42:45<58:36,  1.57s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1408, 'learning_rate': 5.68527918781726e-05, 'epoch': 4.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|âââââ     | 1800/3940 [45:28<57:29,  1.61s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1435, 'learning_rate': 5.431472081218274e-05, 'epoch': 4.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|âââââ     | 1900/3940 [48:14<1:00:06,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1129, 'learning_rate': 5.17766497461929e-05, 'epoch': 4.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|âââââ     | 2000/3940 [50:58<50:43,  1.57s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1164, 'learning_rate': 4.9238578680203045e-05, 'epoch': 5.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|ââââââ    | 2100/3940 [53:43<55:43,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.048, 'learning_rate': 4.67005076142132e-05, 'epoch': 5.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|ââââââ    | 2200/3940 [56:28<52:39,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0399, 'learning_rate': 4.416243654822335e-05, 'epoch': 5.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|ââââââ    | 2300/3940 [59:13<46:38,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0095, 'learning_rate': 4.162436548223351e-05, 'epoch': 5.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|ââââââ    | 2400/3940 [1:02:00<46:28,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0179, 'learning_rate': 3.9086294416243655e-05, 'epoch': 6.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|âââââââ   | 2500/3940 [1:04:47<37:49,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9297, 'learning_rate': 3.654822335025381e-05, 'epoch': 6.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|âââââââ   | 2600/3940 [1:07:30<34:34,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9666, 'learning_rate': 3.401015228426396e-05, 'epoch': 6.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|âââââââ   | 2700/3940 [1:10:17<34:54,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9393, 'learning_rate': 3.147208121827411e-05, 'epoch': 6.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|âââââââ   | 2800/3940 [1:13:05<30:29,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9286, 'learning_rate': 2.8934010152284264e-05, 'epoch': 7.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|ââââââââ  | 2900/3940 [1:15:51<29:27,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8979, 'learning_rate': 2.6395939086294418e-05, 'epoch': 7.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|ââââââââ  | 3000/3940 [1:18:41<25:27,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8896, 'learning_rate': 2.385786802030457e-05, 'epoch': 7.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|ââââââââ  | 3100/3940 [1:21:30<22:13,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9174, 'learning_rate': 2.1319796954314723e-05, 'epoch': 7.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|ââââââââ  | 3200/3940 [1:24:15<20:19,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8749, 'learning_rate': 1.8781725888324874e-05, 'epoch': 8.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|âââââââââ | 3300/3940 [1:27:05<19:50,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8756, 'learning_rate': 1.6243654822335024e-05, 'epoch': 8.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|âââââââââ | 3400/3940 [1:29:47<14:30,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.848, 'learning_rate': 1.3705583756345178e-05, 'epoch': 8.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|âââââââââ | 3500/3940 [1:32:34<12:30,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8546, 'learning_rate': 1.116751269035533e-05, 'epoch': 8.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|ââââââââââ| 3600/3940 [1:35:24<10:16,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8297, 'learning_rate': 8.629441624365483e-06, 'epoch': 9.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|ââââââââââ| 3700/3940 [1:38:11<06:29,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.841, 'learning_rate': 6.091370558375635e-06, 'epoch': 9.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|ââââââââââ| 3800/3940 [1:40:57<03:29,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8375, 'learning_rate': 3.5532994923857873e-06, 'epoch': 9.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|ââââââââââ| 3900/3940 [1:43:45<01:07,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8366, 'learning_rate': 1.015228426395939e-06, 'epoch': 9.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 3940/3940 [1:44:52<00:00,  1.82s/it][INFO|trainer.py:1885] 2023-10-02 00:02:24,755 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|ââââââââââ| 3940/3940 [1:44:53<00:00,  1.60s/it]\n",
      "[INFO|trainer.py:2693] 2023-10-02 00:02:24,909 >> Saving model checkpoint to models/t5-qg-bobby-2023-10-01\n",
      "[INFO|configuration_utils.py:447] 2023-10-02 00:02:24,912 >> Configuration saved in models/t5-qg-bobby-2023-10-01\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 6293.1183, 'train_samples_per_second': 10.025, 'train_steps_per_second': 0.626, 'train_loss': 1.2286052394034295, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:1637] 2023-10-02 00:02:28,140 >> Model weights saved in models/t5-qg-bobby-2023-10-01\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2157] 2023-10-02 00:02:28,142 >> tokenizer config file saved in models/t5-qg-bobby-2023-10-01\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2164] 2023-10-02 00:02:28,143 >> Special tokens file saved in models/t5-qg-bobby-2023-10-01\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =       10.0\n",
      "  train_loss               =     1.2286\n",
      "  train_runtime            = 1:44:53.11\n",
      "  train_samples            =       6309\n",
      "  train_samples_per_second =     10.025\n",
      "  train_steps_per_second   =      0.626\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "if training_args.do_train:\n",
    "    checkpoint = None\n",
    "    if training_args.resume_from_checkpoint is not None:\n",
    "        checkpoint = training_args.resume_from_checkpoint\n",
    "    elif last_checkpoint is not None:\n",
    "        checkpoint = last_checkpoint\n",
    "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "    trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "\n",
    "    metrics = train_result.metrics\n",
    "    max_train_samples = (\n",
    "        data_args.max_train_samples\n",
    "        if data_args.max_train_samples is not None\n",
    "        else len(train_dataset)\n",
    "    )\n",
    "    metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "\n",
    "# Evaluation\n",
    "results = {}\n",
    "max_length = (\n",
    "    training_args.generation_max_length\n",
    "    if training_args.generation_max_length is not None\n",
    "    else data_args.val_max_target_length\n",
    ")\n",
    "num_beams = (\n",
    "    data_args.num_beams\n",
    "    if data_args.num_beams is not None\n",
    "    else training_args.generation_num_beams\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_args.do_eval:\n",
    "    logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "    metrics = trainer.evaluate(\n",
    "        max_length=max_length, num_beams=num_beams, metric_key_prefix=\"eval\"\n",
    "    )\n",
    "    max_eval_samples = (\n",
    "        data_args.max_eval_samples\n",
    "        if data_args.max_eval_samples is not None\n",
    "        else len(eval_dataset)\n",
    "    )\n",
    "    metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n",
    "\n",
    "    trainer.log_metrics(\"eval\", metrics)\n",
    "    trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/02/2023 00:02:28 - INFO - __main__ - *** Predict ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:2944] 2023-10-02 00:02:28,205 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2946] 2023-10-02 00:02:28,206 >>   Num examples = 702\n",
      "[INFO|trainer.py:2949] 2023-10-02 00:02:28,207 >>   Batch size = 2\n",
      "100%|ââââââââââ| 351/351 [04:02<00:00,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/02/2023 00:06:31 - INFO - datasets.metric - Removing C:\\Users\\Adam\\.cache\\huggingface\\metrics\\sacrebleu\\default\\default_experiment-1-0.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 351/351 [04:02<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** test metrics *****\n",
      "  predict_bleu               =    21.8433\n",
      "  predict_gen_len            =    17.6325\n",
      "  predict_loss               =     1.0902\n",
      "  predict_runtime            = 0:04:03.29\n",
      "  predict_samples            =        702\n",
      "  predict_samples_per_second =      2.885\n",
      "  predict_steps_per_second   =      1.443\n"
     ]
    }
   ],
   "source": [
    "if training_args.do_predict:\n",
    "    logger.info(\"*** Predict ***\")\n",
    "\n",
    "    predict_results = trainer.predict(\n",
    "        predict_dataset,\n",
    "        metric_key_prefix=\"predict\",\n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,\n",
    "    )\n",
    "    predictions = [\n",
    "        tokenizer.decode(ids, skip_special_tokens=True)\n",
    "        for ids in predict_results.predictions\n",
    "    ]\n",
    "    with open(f\"{training_args.output_dir}/predictions.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(predictions))\n",
    "    sources = [\n",
    "        tokenizer.decode(\n",
    "            list(map(lambda t: 0 if t == -100 else t, ids)),\n",
    "            skip_special_tokens=True,\n",
    "        )\n",
    "        for ids in predict_results.label_ids\n",
    "    ]\n",
    "    with open(f\"{training_args.output_dir}/labels.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(sources))\n",
    "    metrics = predict_results.metrics\n",
    "    max_predict_samples = (\n",
    "        data_args.max_predict_samples\n",
    "        if data_args.max_predict_samples is not None\n",
    "        else len(predict_dataset)\n",
    "    )\n",
    "    metrics[\"predict_samples\"] = min(max_predict_samples, len(predict_dataset))\n",
    "\n",
    "    trainer.log_metrics(\"test\", metrics)\n",
    "    trainer.save_metrics(\"test\", metrics)\n",
    "\n",
    "    if trainer.is_world_process_zero():\n",
    "        if training_args.predict_with_generate:\n",
    "            predictions = tokenizer.batch_decode(\n",
    "                predict_results.predictions,\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=True,\n",
    "            )\n",
    "            predictions = [pred.strip() for pred in predictions]\n",
    "            output_prediction_file = os.path.join(\n",
    "                training_args.output_dir, \"generated_predictions.txt\"\n",
    "            )\n",
    "            with open(output_prediction_file, \"w\", encoding=\"utf-8\") as writer:\n",
    "                writer.write(\"\\n\".join(predictions))\n",
    "\n",
    "kwargs = {\n",
    "    \"finetuned_from\": model_args.model_name_or_path,\n",
    "    \"tasks\": \"question generation\",\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
